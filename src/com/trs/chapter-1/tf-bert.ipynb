{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca86c2eb-69e6-4760-8f14-de76d6649517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ[\"https_proxy\"] = \"http://192.168.5.68:38080\"\n",
    "# os.environ[\"https_proxy\"] =\"\"\n",
    "os.environ[\"http_proxy\"] = \"\"\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "\n",
    "# print(torch.cuda.current_device)\n",
    "# torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba73122d-38cb-409f-91db-79dd418d5275",
   "metadata": {},
   "source": [
    "## step1 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2239804-a904-4781-bfa8-56caeeabb11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import AutoModel,AutoModelForSequenceClassification,BertForSequenceClassification\n",
    "from datasets import load_dataset,Dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3404d778-ad0b-4378-9cd0-ff59539b1db5",
   "metadata": {},
   "source": [
    "## Step2 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de0ce0f8-c81e-4808-bc50-fe3a15ac4c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_data =pd.read_excel(\"./data/emotion_tag_hjy_0506.xls\")\n",
    "pd_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab584d3-f78b-4480-b875-06ea2bf6df78",
   "metadata": {},
   "source": [
    "## Step3 数据处理转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f5edea2-d293-414d-9f50-78ad6027c860",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m\n\u001b[1;32m      1\u001b[0m tag_to_label \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m悲伤\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m愤怒\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m厌恶\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m6\u001b[39m\n\u001b[1;32m      9\u001b[0m     }\n\u001b[1;32m     11\u001b[0m label_to_tag \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m          \u001b[38;5;241m0\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m悲伤\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m          \u001b[38;5;241m1\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m愤怒\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m          \u001b[38;5;241m6\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m厌恶\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m     }\n\u001b[0;32m---> 20\u001b[0m pd_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[43mpd_data\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtag\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: tag_to_label[x])\n\u001b[1;32m     21\u001b[0m pd_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent_cn\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mpd_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_cn\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m c: c\u001b[38;5;241m.\u001b[39mstrip())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd_data' is not defined"
     ]
    }
   ],
   "source": [
    "tag_to_label = {\n",
    "        \"悲伤\": 0,\n",
    "        \"愤怒\": 1,\n",
    "        \"惊讶\": 2,\n",
    "        \"恐惧\": 3,\n",
    "        \"快乐\": 4,\n",
    "        \"其他\": 5,\n",
    "        \"厌恶\": 6\n",
    "    }\n",
    "\n",
    "label_to_tag = {\n",
    "         0:\"悲伤\",\n",
    "         1:\"愤怒\",\n",
    "         2:\"惊讶\",\n",
    "         3:\"恐惧\",\n",
    "         4:\"快乐\",\n",
    "         5:\"其他\",\n",
    "         6:\"厌恶\"\n",
    "    }\n",
    "pd_data['label']=pd_data[\"tag\"].map(lambda x: tag_to_label[x])\n",
    "pd_data['content_cn']=pd_data[\"content_cn\"].map(lambda c: c.strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e70a619-760f-4c34-92d3-106df1cfa27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79a284b7a6a4fc7b7bae4158bc818f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65893809a2d8488890c813d2171498e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(pd_data)\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.filter(lambda x: x[\"content_cn\"] is not None)\n",
    "dataset = dataset.filter(lambda x: x[\"label\"] is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b369b5f9-3ed4-44cd-8e53-83b33b7fe8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ir_sid', 'content_cn', 'tag', 'label'],\n",
       "        num_rows: 1260\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ir_sid', 'content_cn', 'tag', 'label'],\n",
       "        num_rows: 140\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.train_test_split(test_size =0.1)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433440a8-7c92-462e-9abf-e401d6781d4b",
   "metadata": {},
   "source": [
    "## Step4 模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1df81c98-e412-49d7-ac8a-0d71e9e49a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-macbert-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='hfl/chinese-macbert-large', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "mode_name=\"hfl/chinese-macbert-large\"\n",
    "# mode_name=\"google-bert/bert-base-chinese\"\n",
    "# model = AutoModel.from_pretrained(mode_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(mode_name,num_labels=len(label_to_tag),\n",
    "        label2id=tag_to_label,\n",
    "        # problem_type = \"single_label_classification\",\n",
    "        id2label=label_to_tag)\n",
    "tokenizer = AutoTokenizer.from_pretrained(mode_name)\n",
    "tokenizer\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f777a3-b06f-4a6a-babd-d27852c1d69e",
   "metadata": {},
   "source": [
    "## Step5 tokenizer 处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "669d5d04-f5fc-44c3-92f3-dee437f1789d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1528ef591b994471bd4fa17108f9a9ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf841ba2f54646b88fdd51587203fb57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] 已 经 执 政 四 分 之 一 世 纪 的 普 京 再 度 以 压 倒 性 优 势 赢 得 选 举 。 虽 然 西 方 指 责 反 对 派 缺 席 的 这 场 选 举 不 公 平 ， 但 如 此 高 的 得 票 率 还 是 表 明 了 民 众 对 普 京 的 支 持 和 认 可 。 为 什 么 有 这 个 结 果 ？ 主 要 有 两 个 原 因 。 一 ， 是 民 众 对 叶 利 钦 时 代 的 苦 难 依 然 记 忆 犹 新 。 叶 利 钦 执 政 八 年 ， 按 西 方 的 标 准 是 最 [UNK] 民 主 [UNK] 的 时 期 ， 也 是 与 西 方 关 系 最 好 的 时 期 。 但 就 是 这 个 时 期 带 给 俄 罗 斯 无 穷 无 尽 的 灾 难 ： 经 济 崩 溃 、 人 均 gdp 下 降 50 % 以 上 、 通 货 膨 胀 超 过 2000 % ， 民 众 多 年 的 积 蓄 一 夜 之 间 化 为 乌 有 、 土 地 广 袤 的 国 家 70 % 的 食 品 竟 然 依 靠 进 口 。 在 这 样 的 状 况 下 ， 苏 联 时 期 建 立 的 社 会 保 险 、 免 费 医 疗 等 也 形 同 虚 设 。 根 据 联 合 国 粮 农 组 织 标 准 ， 恩 格 尔 系 数 50 % - 60 % 为 温 饱 ； 30 % - 40 % 为 相 对 富 裕 。 在 1990 年 以 前 ， 俄 罗 斯 恩 格 尔 系 数 为 30 % ， 而 1995 年 为 53 % ， 整 个 国 家 从 相 对 富 裕 降 为 温 饱 。 至 于 连 养 老 金 都 领 不 到 的 民 众 ， 温 饱 都 是 奢 求 。 西 方 民 主 之 于 俄 猛 于 病 毒 于 是 在 俄 罗 斯 ， 出 现 各 种 难 以 想 像 的 情 景 ： 前 往 车 臣 作 战 的 军 人 连 基 本 生 活 费 都 发 不 出 ； 民 众 买 烟 居 然 是 一 根 一 根 的 买 ； 俄 罗 斯 出 现 了 人 类 历 史 上 罕 见 的 和 平 时 期 人 均 寿 命 大 幅 下 降 的 现 象 （ 下 降 3. 5 岁 ） ， 尤 其 是 男 性 竟 然 为 57 岁 ， 低 于 领 退 休 金 的 年 龄 。 幅 度 超 过 美 国 混 乱 的 疫 情 时 期 （ 2. 7 岁 ） 。 俄 罗 斯 [UNK] 民 主 [UNK] 真 可 谓 猛 于 病 毒 [SEP]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def data_procce_function(example):\n",
    "    token_exmaple = tokenizer(example['content_cn'],max_length=512,truncation=True)\n",
    "    token_exmaple['labels'] = example['label']\n",
    "    return token_exmaple\n",
    "\n",
    "\n",
    "token_dataset = dataset.map(function=data_procce_function,batched=True,remove_columns=dataset[\"train\"].column_names)\n",
    "# tokenizer\n",
    "token_dataset\n",
    "# token_dataset[\"train\"][0]\n",
    "tokenizer.decode(token_dataset[\"train\"][0]['input_ids'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19664a7-cb2f-4210-9fca-e72ccdab021b",
   "metadata": {},
   "source": [
    "## Step6 创建评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66e9d708-af6e-4a58-b2ec-8e370288de53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate,numpy \n",
    "\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "# f1_mectric = evaluate.load(\"f1\")\n",
    "def eval_metric(eval_predict):\n",
    "    predictions,labels = eval_predict\n",
    "    # print(predictions)\n",
    "    # predictions = predictions.argmax(axis=-1)\n",
    "    predictions = numpy.argmax(predictions, axis=-1)\n",
    "    result = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    # if len(result) > 1:\n",
    "    #     result[\"combined_score\"] = numpy.mean(list(result.values())).item()\n",
    "    return result\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb4607b-57cc-4efc-a984-f43bb5024995",
   "metadata": {},
   "source": [
    "## Step7 创建训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3489f08e-55c1-4a44-a14e-a05afe00dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "                               \n",
    "                               output_dir=\"./output\",\n",
    "                               # per_device_eval_batch_size=64,\n",
    "                               # per_device_train_batch_size=4,\n",
    "                               logging_steps=50,\n",
    "                               # logging_dir='./logs',\n",
    "                               logging_strategy='steps',\n",
    "                               # logging_first_step=True,\n",
    "                               # save_strategy=\"epoch\",\n",
    "                               # use_mps_device=\"2\",\n",
    "                               num_train_epochs=5,  \n",
    "                               # learning_rate=2e-5,\n",
    "                               # weight_decay=0.01,\n",
    "                               evaluation_strategy=\"steps\",\n",
    "                               metric_for_best_model=\"accuracy\",\n",
    "                               load_best_model_at_end=True\n",
    "\n",
    "                               # per_device_train_batch_size=1,   # 训练时的batch_size\n",
    "                               # gradient_accumulation_steps=32,  # *** 梯度累加 ***\n",
    "                               # gradient_checkpointing=True,     # *** 梯度检查点 ***\n",
    "                               # optim=\"adafactor\",               # *** adafactor优化器 *** \n",
    "                               # per_device_eval_batch_size=1,    # 验证时的batch_size\n",
    "                               # num_train_epochs=1,              # 训练轮数\n",
    "                               # logging_steps=10,                # log 打印的频率\n",
    "                               # evaluation_strategy=\"epoch\",     # 评估策略\n",
    "                               # save_strategy=\"epoch\",           # 保存策略\n",
    "                               # save_total_limit=3,              # 最大保存数\n",
    "                               # learning_rate=2e-5,              # 学习率\n",
    "                               # weight_decay=0.01,               # weight_decay\n",
    "                               # metric_for_best_model=\"f1\",      # 设定评估指标\n",
    "                               # load_best_model_at_end=True\n",
    "                              )\n",
    "# train_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba86e7c6-19f5-4754-9d04-35886b4febe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='790' max='790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [790/790 08:51, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.947100</td>\n",
       "      <td>1.831960</td>\n",
       "      <td>0.278571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.581200</td>\n",
       "      <td>1.484488</td>\n",
       "      <td>0.464286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.364500</td>\n",
       "      <td>1.341561</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.155600</td>\n",
       "      <td>1.130627</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.025100</td>\n",
       "      <td>1.032243</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.037900</td>\n",
       "      <td>0.976122</td>\n",
       "      <td>0.707143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.717800</td>\n",
       "      <td>1.057946</td>\n",
       "      <td>0.671429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.556900</td>\n",
       "      <td>1.180457</td>\n",
       "      <td>0.678571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.600900</td>\n",
       "      <td>1.072801</td>\n",
       "      <td>0.707143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.334400</td>\n",
       "      <td>1.474848</td>\n",
       "      <td>0.657143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.310900</td>\n",
       "      <td>1.403893</td>\n",
       "      <td>0.692857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.160500</td>\n",
       "      <td>1.322764</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.178200</td>\n",
       "      <td>1.372251</td>\n",
       "      <td>0.721429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.088300</td>\n",
       "      <td>1.469332</td>\n",
       "      <td>0.707143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.080200</td>\n",
       "      <td>1.524589</td>\n",
       "      <td>0.707143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer,DataCollatorWithPadding\n",
    "import os\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "# torch.backends.cudnn.enable =True\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "trainer = Trainer(model=model,\n",
    "               args=train_args,\n",
    "               train_dataset=token_dataset['train'],\n",
    "               eval_dataset=token_dataset['test'],\n",
    "               data_collator=data_collator,\n",
    "               compute_metrics=eval_metric\n",
    "                )\n",
    "train_result = trainer.train()\n",
    "\n",
    "# for obj in trainer.state.log_history:\n",
    "#     logging.info(str(obj))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c181723-a0d0-4194-ac16-98c3429aa032",
   "metadata": {},
   "source": [
    "## Step8模型评估\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57d6fab4-db8a-4b0d-87b6-128f08240cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  total_flos               =  5468043GF\n",
      "  train_loss               =     0.7113\n",
      "  train_runtime            = 0:08:52.56\n",
      "  train_samples_per_second =     11.829\n",
      "  train_steps_per_second   =      1.483\n"
     ]
    }
   ],
   "source": [
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824ac0c5-c828-4752-9896-d0fceacb5485",
   "metadata": {},
   "source": [
    "## Step 9 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6f3ca6f-5c61-4ab0-a348-306cdae25649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '恐惧', 'score': 0.9721205234527588}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "#input='4月10日｜阿里巴巴(9988.HK)今日表现强势，盘中最高涨5.82%报74.6港元，股价创3月14日以来新高。消息面上，阿里巴巴创始人马云于阿里内网发帖，高度肯定蔡崇信和吴泳铭组成的新管理层的变革勇气，称阿里巴巴已重回健康成长轨道，并支持继续改革。这是退休后的马云五年来首次长篇幅分享对公司改革创新及未来前景的思考。'\n",
    "#input='这位辛迪·坎宁安明星说，当被告知她在28年后被解雇时，她“心碎”了......阅读整个故事'\n",
    "#input='阿尔贝松杂货店似乎发现自己成为了更多抖音诽谤的接收者，但这一次，该应用程序的一些人认为DoorDash可能与一名用户的碎牛肉剧有关。Shantaq(@heyshantaqtv)上传了一段病毒视频，呼吁阿尔贝松和...阅读整个故事'\n",
    "input='洪水桥房协地盘四级火，起火逾18小时仍然未救熄。　　消防早上继续灌救，不断向洪雅路地盘火场射水，地盘仍然冒起浓烟，出动无人机升空视察火场情况。　　两个天秤一度被指有倒塌风险，屋宇署评估后沒有即时危险。　　有附近居民指有浓烟涌入家中，需关窗和戴上口罩。洪福邨居民何先生：「烟味很臭，我住在2楼也嗅到烟味，很像胶味、刺喉。今天好一些、昨天（星期二）厉害。昨天我不敢上去。」洪福邨居民黄小姐：「嗅到、仍很大烟味。担心味道、始终它烧了很久，烟也真的大。（家中有甚么应对措施？）把窗全都关掉、开冷气'\n",
    "\n",
    "pipe =pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0)\n",
    "pipe(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce740c5e-25bc-4220-a491-7c6d801d0398",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
